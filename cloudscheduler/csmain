#!/usr/bin/python3

import os
import sys
import json
import math
import time
import logging
import signal
import socket
import multiprocessing
from multiprocessing import Process
from collections import defaultdict

from cloudscheduler.lib.db_config import Config
from cloudscheduler.lib.view_utils import qt
from cloudscheduler.lib.log_tools import get_frame_info
from cloudscheduler.lib.ec2_translations import get_ami_dictionary
from cloudscheduler.lib.ProcessMonitor import ProcessMonitor, check_pid, terminate
from cloudscheduler.lib.poller_functions import start_cycle, wait_cycle

import openstackcloud
import localhostcloud
import ec2cloud

import htcondor
import classad
import boto3

from keystoneclient.auth.identity import v2, v3
from keystoneauth1 import session
from keystoneauth1 import exceptions
from novaclient import client as novaclient



### DB PERMISSIONS STUFF
#    .db_query('view_available_resources')


# Supporting functions
def _get_nova_client(session, region=None):
    nova = novaclient.Client("2", session=session, region_name=region, timeout=10)
    return nova

def _get_openstack_session(cloud):
    authsplit = cloud["authurl"].split('/')
    try:
        version = int(float(authsplit[-1][1:])) if len(authsplit[-1]) > 0 else int(float(authsplit[-2][1:]))
    except ValueError:
        logging.error("Bad OpenStack URL, could not determine version, skipping %s", cloud["authurl"])
        return False
    if version == 2:
        session = _get_openstack_session_v1_v2(
            auth_url=cloud["authurl"],
            username=cloud["username"],
            password=cloud["password"],
            project=cloud["project"])
    else:
        session = _get_openstack_session_v1_v2(
            auth_url=cloud["authurl"],
            username=cloud["username"],
            password=cloud["password"],
            project=cloud["project"],
            user_domain=cloud["user_domain_name"],
            project_domain_name=cloud["project_domain_name"])
    if session is False:
        logging.error("Failed to setup session, skipping %s", cloud["cloud_name"])
        if version == 2:
            logging.error("Connection parameters: \n authurl: %s \n username: %s \n project: %s" %
                          (cloud["authurl"], cloud["username"], cloud["project"]))
        else:
            logging.error(
                "Connection parameters: \n authurl: %s \n username: %s \n project: %s \n user_domain: %s \n project_domain: %s" %
                (cloud["authurl"], cloud["username"], cloud["project"], cloud["user_domain_name"], cloud["project_domain_name"]))
    return session


def _get_openstack_session_v1_v2(auth_url, username, password, project, user_domain="Default", project_domain_name="Default"):
    authsplit = auth_url.split('/')
    try:
        version = int(float(authsplit[-1][1:])) if len(authsplit[-1]) > 0 else int(float(authsplit[-2][1:]))
    except ValueError:
        logging.error("Bad openstack URL: %s, could not determine version, aborting session", auth_url)
        return False
    if version == 2:
        try:
            auth = v2.Password(
                auth_url=auth_url,
                username=username,
                password=password,
                tenant_name=project)
            sess = session.Session(auth=auth, verify=config.categories["condor_poller.py"]["cacerts"])
        except Exception as exc:
            logging.error("Problem importing keystone modules, and getting session for grp:cloud - %s::%s" % (auth_url, exc))
            logging.error("Connection parameters: \n authurl: %s \n username: %s \n project: %s", (auth_url, username, project))
            return False
        return sess
    elif version == 3:
        #connect using keystone v3
        try:
            auth = v3.Password(
                auth_url=auth_url,
                username=username,
                password=password,
                project_name=project,
                user_domain_name=user_domain,
                project_domain_name=project_domain_name)
            sess = session.Session(auth=auth, verify=config.categories["condor_poller.py"]["cacerts"])
        except Exception as exc:
            logging.error("Problem importing keystone modules, and getting session for grp:cloud - %s: %s" % (auth_url, exc))
            logging.error("Connection parameters: \n authurl: %s \n username: %s \n project: %s \n user_domain: %s \n project_domain: %s" % (auth_url, username, project, user_domain, project_domain_name))
            return False
        return sess



def _get_ec2_session(cloud):
    return boto3.session.Session(region_name=cloud["region"],
                                 aws_access_key_id=cloud["username"],
                                 aws_secret_access_key=cloud["password"])

def _get_ec2_client(session):
    return session.client('ec2')


def get_condor_session(hostname=None):
    try:
        condor_session = htcondor.Collector(hostname)
        return condor_session
    except Exception as exc:
        logging.exception("Failed to get condor session for %s:" % hostname)
        logging.error(exc)
        return False


def get_master_classad(session, machine, hostname):
    MASTER_TYPE = htcondor.AdTypes.Master
    STARTD_TYPE = htcondor.AdTypes.Startd
    try:
        if machine is not "":
            condor_classad = session.query(MASTER_TYPE, 'Name=="%s"' % machine)[0]
        else:
            condor_classad = session.query(MASTER_TYPE, 'regexp("%s", Name, "i")' % hostname)[0]
        return condor_classad
    except IndexError:
        logging.error("Failed to retrieve classad from condor. No matching classad")
    except Exception as exc:
        logging.error("Failed to retrieve classad from condor. Communication error :")
        logging.error(exc)
        return -1
    return False


def get_startd_classads(session, machine):
    startd_list = []
    STARTD_TYPE = htcondor.AdTypes.Startd
    try:
        condor_classads = session.query(STARTD_TYPE, 'Machine=="%s"' % machine)
        for classad in condor_classads:
            startd_list.append(classad)
        return startd_list
    except Exception as exc:
        logging.error("Failed to retrieve machine classads, aborting...")
        logging.error(exc)
        return False


def invalidate_master_classad(session, classad):
    return session.advertise([classad], "INVALIDATE_MASTER_ADS")

def invalidate_startd_classads(session, classad_list):
    return session.advertise(classad_list, "INVALIDATE_STARTD_ADS")

def available_resources_initialize(config):
    used_resource_list = []
    rc, msg = config.db_execute('select * from view_total_used_resources')
    for row in config.db_cursor:
        used_resource_list.append(row)

    used_resources = {'group_cloud': {}, 'provider': {}}
    for used_resource in used_resource_list:
        provider = '%s|%s|%s' % (used_resource['authurl'], used_resource['region'], used_resource['project'])
        if provider not in used_resources['provider']:
            used_resources['provider'][provider] = {'VMs': 0, 'cores_used': 0, 'disk_used': 0, 'ram_used': 0, 'swap_used': 0}

        used_resources['provider'][provider]['VMs']        += used_resource['VMs']
        used_resources['provider'][provider]['cores_used'] += used_resource['cores']
        used_resources['provider'][provider]['disk_used']  += used_resource['disk']
        used_resources['provider'][provider]['ram_used']   += used_resource['ram']
        used_resources['provider'][provider]['swap_used']  += used_resource['swap']

    for provider in sorted(used_resources['provider']):
        log.debug("available_resources_initialize, VMs: %11s, cores: %11s, disk: %11s, RAM: %11s, provider: %s" % (
            used_resources['provider'][provider]['VMs'],
            used_resources['provider'][provider]['cores_used'],
            used_resources['provider'][provider]['disk_used'],
            used_resources['provider'][provider]['ram_used'],
            provider
            ))

    return used_resources

def available_resources_query(used_resources, available_resource):
    def set_default(ctl, default_value):
        if ctl < 0:
            return default_value
        else:
            return ctl

    group_cloud = '%s::%s' % (available_resource['group_name'], available_resource['cloud_name'])
    if group_cloud not in used_resources['group_cloud']:
        used_resources['group_cloud'][group_cloud] = {
            'VMs': available_resource['VMs'],
            'cores_used': available_resource['cores_used'],
            'disk_used': available_resource['disk_used'],
            'ram_used': available_resource['ram_used'],
            'swap_used': available_resource['swap_used'],
            }

    provider = '%s|%s|%s' % (available_resource['authurl'], available_resource['region'], available_resource['project'])
    if provider not in used_resources['provider']:
        used_resources['provider'][provider] = {'VMs': 0, 'cores_used': 0, 'disk_used': 0, 'ram_used': 0, 'swap_used': 0}

    cores_limit = min(
        set_default(available_resource['cores_ctl'], available_resource['cores_max']),
        set_default(available_resource['cores_softmax'], available_resource['cores_max']) - max(0, used_resources['provider'][provider]['cores_used'] - used_resources['group_cloud'][group_cloud]['cores_used']),
        available_resource['cores_max'] - max(0, used_resources['provider'][provider]['cores_used'] - used_resources['group_cloud'][group_cloud]['cores_used'])
        )

    ram_limit = min(
        set_default(available_resource['ram_ctl'], available_resource['ram_max']),
        available_resource['ram_max'] - max(0, used_resources['provider'][provider]['ram_used'] - used_resources['group_cloud'][group_cloud]['ram_used'])
        )

    log.debug("provider: %s used:%s" % (used_resources['provider'][provider]['VMs'], used_resources['group_cloud'][group_cloud]['VMs'])) 

    slots = min(
        available_resource['VMs_max'] - used_resources['provider'][provider]['VMs'],
        int(max(0, cores_limit - used_resources['group_cloud'][group_cloud]['cores_used']) / available_resource['flavor_cores']),
        int(max(0, ram_limit - used_resources['group_cloud'][group_cloud]['ram_used']) / available_resource['flavor_ram'])
        )

    log.info("available_resources_query(%s), VMs(%s/%s), Cores(%s,%s), RAM(%s,%s) Slots(%s) available for resource: %s" % (
        available_resource['group_name'],
        used_resources['group_cloud'][group_cloud]['VMs'],
        used_resources['provider'][provider]['VMs'],
        used_resources['group_cloud'][group_cloud]['cores_used'],
        cores_limit,
        used_resources['group_cloud'][group_cloud]['ram_used'],
        ram_limit,
        slots,
        available_resource['flavor']
        ))
    return slots

def available_resources_update(used_resources, available_resource, consumed_slots):
    consumed_cores = available_resource['flavor_cores'] * consumed_slots
    consumed_ram = available_resource['flavor_ram'] * consumed_slots

    group_cloud = '%s::%s' % (available_resource['group_name'], available_resource['cloud_name'])
    if group_cloud not in used_resources['group_cloud']:
        used_resources['group_cloud'][group_cloud] = {
            'VMs': available_resource['VMs'],
            'cores_used': available_resource['cores_used'],
            'disk_used': available_resource['disk_used'],
            'ram_used': available_resource['ram_used'],
            'swap_used': available_resource['swap_used'],
            }

    provider = '%s|%s|%s' % (available_resource['authurl'], available_resource['region'], available_resource['project'])
    if provider not in used_resources['provider']:
        used_resources['provider'][provider] = {'VMs': 0, 'cores_used': 0, 'disk_used': 0, 'ram_used': 0, 'swap_used': 0}

    pre_group_VMs = used_resources['group_cloud'][group_cloud]['VMs']
    pre_group_cores = used_resources['group_cloud'][group_cloud]['cores_used']
    pre_group_ram = used_resources['group_cloud'][group_cloud]['ram_used']

    used_resources['group_cloud'][group_cloud]['VMs'] += consumed_slots
    used_resources['group_cloud'][group_cloud]['cores_used'] += consumed_cores
    used_resources['group_cloud'][group_cloud]['ram_used'] += consumed_ram

    pre_provider_VMs = used_resources['provider'][provider]['VMs']
    pre_provider_cores = used_resources['provider'][provider]['cores_used']
    pre_provider_ram = used_resources['provider'][provider]['ram_used']

    used_resources['provider'][provider]['VMs'] += consumed_slots
    used_resources['provider'][provider]['cores_used'] += consumed_cores
    used_resources['provider'][provider]['ram_used'] += consumed_ram

    log.debug("available_resources_update(%s, %s), VMs(%s/%s, %s/%s), Cores(%s/%s, %s/%s), RAM(%s/%s, %s/%s) Slots(%s)" % (
        available_resource['group_name'],
        available_resource['flavor'],

        pre_group_VMs,
        used_resources['group_cloud'][group_cloud]['VMs'],
        pre_provider_VMs,
        used_resources['provider'][provider]['VMs'],

        pre_group_cores,
        used_resources['group_cloud'][group_cloud]['cores_used'],
        pre_provider_cores,
        used_resources['provider'][provider]['cores_used'],

        pre_group_ram,
        used_resources['group_cloud'][group_cloud]['ram_used'],
        pre_provider_ram,
        used_resources['provider'][provider]['ram_used'],

        consumed_slots,
        ))

def check_view_idle_vms():
    """Query view_idle_vms and retire as needed."""
    multiprocessing.current_process().name = "csmain_idle_vms"
    log = logging.getLogger(__name__)
    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', 'csmain',
                    pool_size=15)
    if not config:
        print("Problem loading config file.")
        return
    while True:
        config.db_open()
        config.refresh()
        log.debug("-------------------check_view_idle_vms-------------------")
        try:
            where_clause = "retire=0 and terminate=0"
            rc, msg, results = config.db_query("view_idle_vms", where=where_clause)
            vmids = []
            for res in results:
                vmids.append(res["vmid"])
            if vmids:
                log.debug("Setting retire flag on %s VMs.", len(vmids))
                vms = "csv2_vms"
                where_clause = "vmid in ("
                for id in vmids:
                    where_clause += "'%s'," % id
                #replace last , with closing bracket
                where_clause = where_clause[:-1] + ")"
                rc, msg, update_result = config.db_query(vms, where=where_clause)
                for row in update_result:
                    row["retire"] = 1
                    old_updater = row["updater"]
                    row["updater"] = get_frame_info() + ":r1"
                    config.db_merge(vms, row)
                    log.debug("Set retire flag on %s, previous updater: %s",
                              row["hostname"], old_updater)
            config.db_close(commit=True)
        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
            config.db_close()
        time.sleep(config.categories['csmain']['sleep_interval_main_long'])


def _exit(signal_num, frame):
    logging.info("Got signal: %s, exiting.." % signal_num)
    exit(0)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
def process_group_cloud_terminates(pair, config):
    group_name = pair["group_name"]
    cloud_name = pair["cloud_name"]

    #config.db_open()

    VM = "csv2_vms"
    CLOUD = "csv2_clouds"

    terminate_off = config.categories[os.path.basename(sys.argv[0])]["terminate_off"]

    master_type = htcondor.AdTypes.Master
    startd_type = htcondor.AdTypes.Startd

    logging.debug("Processing commands for group:%s, cloud:%s" % (group_name, cloud_name))

    # Terminate code:
    master_list = []
    startd_list = []
    where_clause = "cloud_name='%s' and group_name='%s' and terminate>=1" % (cloud_name, group_name)
    rc, msg, redundant_machine_list = config.db_query("view_condor_host", where=where_clause)
    logging.debug("Actionable VMS:")
    for resource in redundant_machine_list:
        if resource["dynamic_slots"] is not None:
            if resource["primary_slots"] is not None:
                if resource["terminate"] == 1 and resource["dynamic_slots"] >= 1 and resource["primary_slots"] >=1:
                    logging.info("VM still has active slots, skipping terminate on %s" % resource["vmid"])
                    continue


        # we need the relevent vm row to check if its in manual mode and if not, terminate and update termination status
        try:
            where_clause = "group_name='%s' and cloud_name='%s' and vmid='%s'" % (resource["group_name"], resource["cloud_name"], resource["vmid"])
            rc, msg, vm_rows = config.db_query(VM, where=where_clause)
            vm_row = vm_rows[0]
        except Exception as exc:
            logging.error("Unable to retrieve VM row for vmid: %s, skipping terminate..." % resource.vmid)
            continue
        if vm_row["manual_control"] == 1:
            logging.info("VM %s under manual control, skipping terminate..." % resource["vmid"])
            continue


        # Get session with hosting cloud.
        where_clause = "group_name='%s' and cloud_name='%s'" % (vm_row["group_name"], vm_row["cloud_name"])
        rc, msg, clouds = config.db_query(CLOUD, where=where_clause)
        cloud=clouds[0]

        if cloud["cloud_type"] == "openstack":
            session = _get_openstack_session(cloud)
            if session is False:
                continue
         
            if terminate_off:
                logging.critical("Terminates disabled, normal operation would terminate %s" % vm_row["hostname"])
                continue

            # terminate the vm
            nova = _get_nova_client(session, region=cloud["region"])
            try:
                # may want to check for result here Returns: An instance of novaclient.base.TupleWithMeta so probably not that useful
                vm_row["terminate"] = vm_row["terminate"] + 1
                old_updater = vm_row["updater"]
                vm_row["updater"] = str(get_frame_info() + ":t+")

                try:
                    nova.servers.delete(vm_row["vmid"])
                except novaclient.exceptions.NotFound:
                    logging.error("VM not found on cloud, deleting vm entry %s" % vm_row["vmid"])
                    config.db_delete(VM, vm_row)
                    try:
                        config.db_commit()
                    except Exception as exc:
                        logging.exception("Failed to commit vm delete, aborting cycle...")
                        logging.error(exc)
                        break
                    continue #skip rest of logic since this vm is gone anywheres 
                        

                except Exception as exc:
                    logging.error("Unable to delete vm, vm doesnt exist or openstack failure:")
                    exc_type, exc_obj, exc_tb = sys.exc_info()
                    logging.error(exc_type)
                    logging.error(exc)
                    continue
                logging.info("VM Terminated(%s): %s primary slots: %s dynamic slots: %s, last updater: %s" % (vm_row["terminate"], vm_row["hostname"], vm_row["htcondor_partitionable_slots"], vm_row["htcondor_dynamic_slots"], old_updater))
                config.db_merge(VM, vm_row)
                # log here if terminate # /10 = remainder zero
                if vm_row["terminate"] %10 == 0:
                    logging.critical("%s failed terminates on %s user action required" % (vm_row["terminate"] - 1, vm_row["hostname"]))
            except Exception as exc:
                logging.error("Failed to terminate VM: %s, terminates issued: %s" % (vm_row["hostname"], vm_row["terminate"] - 1))
                logging.error(exc)

            # Now that the machine is terminated, we can speed up operations by invalidating the related classads
            # double check that a condor_session exists
            if 'condor_session' not in locals():
                condor_session = get_condor_session()
            if resource["machine"] is not None:
                logging.info("Removing classads for machine %s" % resource["machine"])
            else:
                logging.info("Removing classads for machine %s" % resource["hostname"])
            try:
                master_classad = get_master_classad(condor_session, resource["machine"], resource["hostname"])
                if not master_classad:
                    # there was no matching classad
                    logging.error("Classad not found for %s//%s" % (resource["machine"], resource["hostname"]))

                master_result = invalidate_master_classad(condor_session, master_classad)
                logging.debug("Invalidate master result: %s" % master_result)
                startd_classads = get_startd_classads(condor_session, resource["hostname"])
                startd_result = invalidate_startd_classads(condor_session, startd_classads)
                logging.debug("Invalidate startd result: %s" % startd_result)


                #if resource.machine is not None and resource.machine is not "":
                #    condor_classad = condor_session.query(master_type, 'Name=="%s"' % resource["machine"])[0]
                #else:
                #    condor_classad = condor_session.query(master_type, 'regexp("%s", Name, "i")' % resource["hostname"])[0]
                #master_list.append(condor_classad)

                # this could be a list of adds if a machine has many slots
                #condor_classads = condor_session.query(startd_type, 'Machine=="%s"' % resource["hostname"])
                #for classad in condor_classads:
                #    startd_list.append(classad)
            #except IndexError as exc:
            #    pass
            except Exception as exc:
                logging.error("Failed to get condor classads or issue invalidate:")
                logging.error(exc)
                continue

        elif cloud["cloud_type"] == "amazon":
            if terminate_off:
                logging.critical("Terminates disabled, normal operation would terminate %s" % vm_row["hostname"])
                continue
            #terminate the vm
            amz_session = _get_ec2_session(cloud)
            amz_client = _get_ec2_client(amz_session)
            try:
                vm_row.terminate = vm_row["terminate"] + 1
                old_updater = vm_row["updater"]
                vm_row["updater"] = str(get_frame_info() + ":t+")
                #destroy amazon vm, first we'll need to check if its a reservation
                if vm_row["vmid"][0:3].lower() == "sir":
                    #its a reservation just delete it and destroy the vm
                    # not sure what the difference between a client and connection from csv1 is but there is more work to be done here
                    #
                    # need the command to remove reservation:
                    # cancel_spot_instance_requests(list_of_request_ids)
                    #
                    # need to terminate request, and possible image if instance_id isn't empty
                    try:
                        logging.info("Canceling amazon spot price request: %s" % vm_row["vmid"])
                        amz_client.cancel_spot_instance_requests(SpotInstanceRequestIds=[vm_row["vmid"]])
                        if vm_row["instance_id"] is not None:
                            #spot price vm running need to terminate it:
                            logging.info("Terminating amazon vm: %s" % vm_row["instance_id"])
                            amz_client.terminate_instances(InstanceIds=[vm_row["instance_id"]])
                    except Exception as exc:
                        logging.error("Unable to terminate %s due to:" % vm_row["vmid"])
                        logging.error(exc)
                else:
                    #its a regular instance and just terminate it
                    logging.info("Terminating amazon vm: %s" % vm_row["vmid"])
                    amz_client.terminate_instances(InstanceIds=[vm_row["vmid"]])
            except Exception as exc:
                logging.error("Unable to terminate %s due to:" % vm_row["vmid"])
                logging.error(exc)


        else:
            # Other cloud types will need to be implemented here to terminate any vms not from openstack
            logging.info("Vm not from openstack, or amazon cloud, skipping...")
            continue
    try:
        config.db_commit()
    except Exception as exc:
        logging.exception("Failed to commit vm updates, aborting cycle...")
        logging.error(exc)
        return

    logging.debug("Commands complete")
    return


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Main functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def main():
    """
    main function.
    """
    multiprocessing.current_process().name = "csmain"

    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', ['csmain', 'GSI'], pool_size=25)
    if not config:
        print("Problem loading config file.")
    log = logging.getLogger(__name__)

    cs_host = socket.gethostname()
    try:
        cs_host_ip = socket.gethostbyname(cs_host)
    except:
        cs_host_ip = '*** unresolved ***'

    while_counter = 0
    while(True):
        config.db_open()
        config.refresh()
        logging.basicConfig(filename=config.categories['csmain']['log_file'],
                            level=config.categories['csmain']['log_level'],
                            format="%(asctime)s - %(processName)-12s - %(levelname)s - %(message)s")


        # Get the metadata for groups as json selects with mime_types pre-sorted
        # metadata format: { group : { cloud: [(yaml select, mime_type), (yaml select, mime_type) ] } }
        try:
            ec2_image_dict = get_ami_dictionary()
        except Exception as ex:
            log.exception(ex)
            continue

        try:
            rc, msg, g_metadatas = config.db_query("view_metadata_collation_json")
            metadata = json.loads(g_metadatas[0]["group_metadata"])
        except AttributeError as ae:
            log.exception("Problem loading metadata, is there no metadata?")
            metadata = {}

        used_resources = available_resources_initialize(config)

        rc, msg, avail_resources = config.db_query("view_available_resources")
        available_resources_dict = qt(avail_resources,
            keys = {
                'primary': [
                    'group_name',
                    'flavor',
                ]
            }
        )

        try:
            # Booting up new VMs to fill in any free space on available clouds related to idle queued jobs
            # Get the idle jobs for the current group

            # need to transform this into a tiered dictionary keyed on group
            rc, msg, grps_of_idle_jobs = config.db_query("view_groups_of_idle_jobs", order_by="job_priority")
            idle_job_groups = qt(grps_of_idle_jobs,
                 keys = {
                    'primary': [
                        'group_name',
                    ],
                    'list_duplicates': True,
                }
            )
            list_groups_with_idle_jobs = sorted(idle_job_groups.keys())
            list_groups_with_idle_jobs_len = len(list_groups_with_idle_jobs)
            if list_groups_with_idle_jobs_len > 0:
                list_groups_with_idle_jobs_ix = while_counter % list_groups_with_idle_jobs_len
            else:
                list_groups_with_idle_jobs_ix = 0

            log.info('while_counter: %s, groups with idle jobs (%s/%s): %s' % (
                while_counter,
                list_groups_with_idle_jobs_ix,
                list_groups_with_idle_jobs_len,
                list_groups_with_idle_jobs[list_groups_with_idle_jobs_ix:] + list_groups_with_idle_jobs[:list_groups_with_idle_jobs_ix],
                ))

#           cloud_booted_on = set()  #  Setting this up here now so I can check it and skip over things I boot on due to available slots info will be wrong
            cloud_prio = None 
            if list_groups_with_idle_jobs_len > 0:
                for job_group in list_groups_with_idle_jobs[list_groups_with_idle_jobs_ix:] + list_groups_with_idle_jobs[:list_groups_with_idle_jobs_ix]:
#               for job_group in idle_job_groups:
                    ####### log.debug("Proccessing group: %s" % job_group)
                    booted_for_group = False
                    for idle_job in idle_job_groups[job_group]:
                        current_boot_target_alias = idle_job.get("target_alias")
                        ####### log.debug("Idle jobs for group: %s" % idle_job_groups[job_group])
                        if idle_job.get("idle") == 0:
                            continue
                        if booted_for_group:
                            break # this may change in the future once we have smarter booting


                        ####### log.debug("Info for current job: Group: %s, Target: %s, User: %s, Flavors: %s",
                        # flavors is generated line based on which flavor is the best fit and which clouds have available resources
                        # the format is "Group:Cloud:Flavor, Group:Cloud:Flavor" will need to split and use to filter in resources_matching

                        if idle_job.get("flavors"):
                            flavor_list = [x.strip() for x in idle_job.get("flavors").split(',')]
                        else:
                            continue

                        boot_flavor = None
                        for flavor in flavor_list:
                            if available_resources_dict[job_group].get(flavor, None) is not None:
                                #Found flavour with available resources
                                ####### log.debug("Flavor:%s found to have available slots" % flavor)
                                log.info("Flavor:%s found to have available slots" % flavor)
                                boot_flavor = available_resources_dict[job_group].get(flavor, None)
                            else:
                                ####### log.debug("No available slots for flavor: %s, trying next..." % flavor)
                                continue

                            # We have a boot flavor, check that there is available resources and check shortfalls
                            # If we continue here we go on to the next flavor until we run out
                            # boot_flavor = available_resource row with all info that we need to boot a vm

                            available_slots = available_resources_query(used_resources, boot_flavor)
    #                       if available_slots <= 0:
                                ####### log.debug("No available resources to boot %s" % boot_flavor.get("flavor"))

                            try:
                                if idle_job.get("target_alias") is not None: 
                                    where_clause = "group_name='%s' and target_alias='%s'" % (idle_job.get("group_name"), idle_job.get("target_alias"))
                                else:
                                    where_clause = "group_name='%s' and target_alias is NULL" % idle_job.get("group_name")
                                rc, msg, shortfall = config.db_query("view_active_resource_shortfall", where=where_clause)
                            except:
                                shortfall = []


                            if len(shortfall) < 1:
                                log.warning('unable to retrieve view_active_resource_shortfall for group=%s, target_alias=%s, trying next flavor or skiping idle_job_group).',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"))
                                continue

                            if shortfall[0]["shortfall_cores"]<=0 and shortfall[0]["shortfall_disk"]<=0 and shortfall[0]["shortfall_ram"]<=0:
                                log.info('view_active_resource_shortfall for group=%s, target_alias=%s, cores=%s, disk=%s, ram=%s, no shortfall, trying next flavor or skipping idle_job_group.',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"),
                                    shortfall[0]["shortfall_cores"],
                                    shortfall[0]["shortfall_disk"],
                                    shortfall[0]["shortfall_ram"])
                                continue
                                
                            idle_VMs_throttle = max(config.categories['csmain']['idle_VMs_throttle'], int(shortfall[0]["running"]/10))
                            if shortfall[0]["idle"]>=idle_VMs_throttle:
                                log.info('Too many idle VMs: group=%s, target_alias=%s, idle=%s, running=%s, calculated throttle=%s configurred throttle=%s, trying next flavor or skipping idle_job_group.',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"),
                                    shortfall[0]["idle"],
                                    shortfall[0]["running"],
                                    idle_VMs_throttle,
                                    config.categories['csmain']['idle_VMs_throttle'])
                                continue

                            else:
                                ##log.debug('view_active_resource_shortfall for group=%s, target_alias=%s, cores=%s, disk=%s, ram=%s, too few active resources, processing continues...',
                                log.info('view_active_resource_shortfall for group=%s, target_alias=%s, cores=%s, disk=%s, ram=%s, too few active resources, processing continues...',
                                    idle_job.get("group_name"),
                                    idle_job.get("target_alias"),
                                    shortfall[0]["shortfall_cores"],
                                    shortfall[0]["shortfall_disk"],
                                    shortfall[0]["shortfall_ram"])

                                # Looks like we need more VMs, we should have the flavor slots available from the view_available resources row.
                                # Before we boot anything lets check resrouce contention
                                try:
                                    where_clause = "authurl='%s'" % boot_flavor.get("authurl")
                                    rc, msg, resource = config.db_query("view_resource_contention", where=where_clause)
                                except:
                                     resource = []

                                if len(resource) < 1:
                                    resource = [{
                                        "authurl":boot_flavor.get("authurl"),
                                        "VMs": 0,
                                        "starting": 0,
                                        "unregistered": 0,
                                        "idle": 0,
                                        "running": 0,
                                        "retiring": 0,
                                        "manual": 0,
                                        "error": 0
                                    }]

                                    log.debug('No active vms for group=%s, cloud=%s, authurl=%s, assuming no contention).',
                                        boot_flavor.get("group_name"),
                                        boot_flavor.get("cloud_name"),
                                        boot_flavor.get("authurl"))

                                if resource[0]["starting"] + resource[0]["unregistered"] >= config.categories['csmain']['new_VMs_throttle']:
                                    log.info('Resource contention: group=%s, cloud=%s, resource=%s, starting=%s, unregistered=%s, trying next flavor or moving on to next job group.',
                                        boot_flavor.get("group_name"),
                                        boot_flavor.get("cloud_name"),
                                        resource[0]["authurl"],
                                        resource[0]["starting"],
                                        resource[0]["unregistered"])
                                    continue

#                               if idle_job.get("cloud_name") in cloud_booted_on:
#                                   log.debug("Already booted VMs on %s, skipping to next resource..." % boot_flavor.get("cloud_name"))
#                                   continue

                                # Set cloud priority so we can be sure to fill clouds of this priority first.
                                if cloud_prio == None:
                                    cloud_prio = boot_flavor.get("cloud_priority ")# Set the lowest priority
                                elif cloud_prio == boot_flavor.get("cloud_priority"):
                                    pass # keep booting VMs on clouds of the same priority if possible
                                else:
                                    log.info("Cloud: %s has low priority, skipping until high priority clouds full.." % boot_flavor.get("cloud_name"))
                                    continue # if higher priority cloud found skip

                                current_group_name = idle_job.get("group_name")
                                current_boot_cloud = boot_flavor.get("cloud_name")
#                               current_boot_target_alias = idle_job.get("target_alias")
                                current_boot_flavor = boot_flavor.get("flavor")
                                ####### log.debug("Taking a look at booting on: %s, using flavor: %s" % (current_boot_cloud, current_boot_flavor))
                                def no_zero(val):
                                    if int(val) > 0:
                                        return int(val)
                                    else:
                                        return 1
                                shortfall_cores = math.ceil(shortfall[0]["shortfall_cores"]/no_zero(boot_flavor['flavor_cores']))
                                shortfall_disk = math.ceil(shortfall[0]["shortfall_disk"]/no_zero(boot_flavor['flavor_disk']))
                                shortfall_ram = math.ceil(shortfall[0]["shortfall_ram"]/no_zero(boot_flavor['flavor_ram']))
                                shortfall_slots = max(0,
                                    shortfall_cores,
                                    shortfall_disk,
                                    shortfall_ram
                                    )
                                
                                num_vms_to_boot = min(
                                    config.categories['csmain']['max_start_vm_cloud'],
                                    available_slots,
                                    shortfall_slots
                                    )

                                if num_vms_to_boot == 0:
                                    cloud_prio = None # reset cloud prio so if no slots on the higher priority clouds can still boot on lower
                                    log.debug("No Flavor Slots for %s %s, minumum of max_start_vm_cloud(%s), available_slots(%s), shortfall_slots(%s,%s,%s), check the softmax or foreign VMs to try and see why not booting a new VM." % (
                                        boot_flavor.get("group_name"),
                                        boot_flavor.get("flavor"),
                                        config.categories['csmain']['max_start_vm_cloud'],
                                        available_slots,
                                        shortfall_cores,
                                        shortfall_disk,
                                        shortfall_ram
                                        ))
                                    continue

                                log.info("Trying to boot %s VMs (%s, %s); minumum of max_start_vm_cloud(%s), available_slots(%s), shortfall_slots(%s,%s,%s)." % (
                                    num_vms_to_boot,
                                    current_group_name,
                                    current_boot_flavor,
                                    config.categories['csmain']['max_start_vm_cloud'],
                                    available_slots,
                                    shortfall_cores,
                                    shortfall_disk,
                                    shortfall_ram
                                    ))
                                try:
                                    usertmp = idle_job.get("user").split('@')[0]
                                except:
                                    usertmp = idle_job.get("user")
                                try:
                                    cs_condor_host_ip = socket.gethostbyname(boot_flavor.get("htcondor_fqdn"))
                                except:
                                    cs_condor_host_ip = '*** unresolved **'
                                
                                if boot_flavor.get("worker_cert") is not None:
                                    if boot_flavor.get("worker_cert")[-1] == '\n':
                                        cloud_worker_cert = boot_flavor.get("worker_cert")[:-1].replace('\n', '\n        ')
                                    else:
                                        try:
                                            cloud_worker_cert = boot_flavor.get("worker_cert").decode().replace('\n', '\n        ')
                                        except AttributeError:
                                            cloud_worker_cert = boot_flavor.get("worker_cert").replace('\n', '\n        ')
                                else:
                                    cloud_worker_cert = None

                                if boot_flavor.get("worker_key") is not None:
                                    if boot_flavor.get("worker_key")[-1] == '\n':
                                        cloud_worker_key = boot_flavor.get("worker_key")[:-1].replace('\n', '\n        ')
                                    else:
                                        try:
                                            cloud_worker_key = boot_flavor.get("worker_key").decode().replace('\n', '\n        ')
                                        except AttributeError:
                                            cloud_worker_key = boot_flavor.get("worker_key").replace('\n', '\n        ')
                                else:
                                    cloud_worker_key = None

                                template_dict = {'cs_user': usertmp,
                                                 'cs_host': cs_host,
                                                 'cs_host_id': config.csv2_host_id,
                                                 'cs_host_ip': cs_host_ip,
                                                 'cs_group_name': boot_flavor.get("group_name"),
                                                 'cs_condor_host': boot_flavor.get("htcondor_fqdn"),
                                                 'cs_condor_host_ip': cs_condor_host_ip,
                                                 'cs_condor_name': boot_flavor.get("htcondor_container_hostname"),
                                                 'cs_condor_submitters': boot_flavor.get("htcondor_other_submitters"),
                                                 'cs_cloud_alias': idle_job.get("target_alias"),
                                                 'cs_condorworker_cert_days_to_end_of_life': config.categories['GSI']['cert_days_left_good'],
                                                 'cs_condorworker_optional_gsi_messages': config.categories['csmain']['condorworker_optional_gsi_msgs'],
                                                 'cs_condorworker_cert': cloud_worker_cert,
                                                 'cs_condorworker_key': cloud_worker_key}
                                ######## log.debug(template_dict)

                                # Let's try to boot
                                try:
                                    if idle_job.get("image"):
                                        use_image = idle_job.get("image")
                                    elif boot_flavor.get("default_image"):
                                        use_image = boot_flavor.get("default_image")
                                    else:
                                        use_image = None
                                     
                                    current_cloud_type = boot_flavor.get("cloud_type")
                                    if current_cloud_type == 'openstack':
                                        boot_cloud = openstackcloud.OpenStackCloud(config, resource=boot_flavor, metadata=metadata[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")]
                                                 if metadata and boot_flavor.get("group_name") in metadata
                                                 else [])
                                        boot_cloud.vm_create(num=int(num_vms_to_boot),
                                                   flavor=boot_flavor.get("flavor"),
                                                   job=idle_job,
                                                   template_dict=template_dict,
                                                   image=use_image)

                                    elif current_cloud_type == 'amazon':
                                        if idle_job.get("image"):
                                            use_image = ec2_image_dict[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")][idle_job.get("image")]
                                        elif boot_flavor.get("default_image"):
                                            use_image = ec2_image_dict[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")][boot_flavor.get("default_image")]
                                        boot_cloud = ec2cloud.EC2Cloud(config, resource=boot_flavor,
                                                 metadata=metadata[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")]
                                                 if metadata and boot_flavor.get("group_name") in metadata
                                                 else [])
                                        boot_cloud.vm_create(num=int(num_vms_to_boot),
                                                   flavor=boot_flavor.get("flavor"),
                                                   job=idle_job,
                                                   template_dict=template_dict,
                                                   image=use_image)


                                    elif current_cloud_type == 'localhost':
                                        boot_cloud = localhostcloud.LocalHostCloud(config, resource=boot_flavor,
                                                 metadata=metadata[boot_flavor.get("group_name")][boot_flavor.get("cloud_name")]
                                                 if metadata and boot_flavor.get("group_name") in metadata
                                                 else [])
                                        boot_cloud.vm_create(num=int(num_vms_to_boot),
                                                   flavor=boot_flavor.get("flavor"),
                                                   job=idle_job,
                                                   template_dict=template_dict,
                                                   image=use_image)
                                    else:
                                        log.warning('Boot request for invalid cloud type "%s", ignoring.' % current_cloud_type)

                                    ####### log.debug('done booting on cloud %s', boot_flavor.get("cloud_name"))
                                    available_resources_update(used_resources, boot_flavor, num_vms_to_boot)
#                                   cloud_booted_on.add(boot_flavor.get("cloud_name"))
                                    booted_for_group = True
                                    break


                                except Exception as ex:
                                    log.exception("Disable cloud %s due to exception(later).", boot_flavor.get("cloud_name"))

                        if not booted_for_group:
                            log.info("No Flavor Slots for %s, target alias: %s." % (boot_flavor.get("group_name"), current_boot_target_alias))

        except TimeoutError as ex:
            log.exception(ex)
            sys.exit(1)
        except Exception as ex:
            log.exception(ex)
            config.db_close()
            time.sleep(2)
            continue
        config.db_close()
        while_counter += 1
        time.sleep(config.categories['csmain']['sleep_interval_main_long'])


def vm_termination(args_list):
    target_group = args_list[0]
    target_cloud = args_list[1]
    multiprocessing.current_process().name = "VM Termination - %s - %s" % (target_group, target_cloud)

    #over-ride signal handler defined in main
    signal.signal(signal.SIGTERM, _exit)

    # database setup
    config = Config('/etc/cloudscheduler/cloudscheduler.yaml', ['csmain', 'condor_poller.py',  "ProcessMonitor"], pool_size=3, signals=True)
    PID_FILE = config.categories["ProcessMonitor"]["pid_path"] + os.path.basename(sys.argv[0])

    cycle_start_time = 0
    new_poll_time = 0
    poll_time_history = [0,0,0,0]


    cycle_count = 0


    try:
        config.db_open()
        while True:
            logging.debug("Beginning vm retire cycle")
            config.refresh()

            if not os.path.exists(PID_FILE):
                logging.debug("Stop set, exiting...")
                break

#            signal.signal(signal.SIGINT, signal.SIG_IGN)
            new_poll_time, cycle_start_time = start_cycle(new_poll_time, cycle_start_time)
       
            pair = {
                "group_name": target_group,
                "cloud_name": target_cloud,
            }
            process_group_cloud_terminates(pair, config)
            
            try:
                config.db_commit()
            except Exception as exc:
                logging.error("Error during final commit, likely that a vm was removed from database before final terminate update was comitted..")
                logging.exception(exc)

            
#            signal.signal(signal.SIGINT, config.signals['SIGINT'])
            if not os.path.exists(PID_FILE):
                logging.info("Stop set, exiting...")
                break
            wait_cycle(cycle_start_time, poll_time_history, config.categories["condor_poller.py"]["sleep_interval_command"], config)

    except Exception as exc:
        logging.exception("Command consumer while loop exception, process terminating...")
        logging.error(exc)

#~~~~~~~~~~~~~~~~~






if __name__ == '__main__':

    process_ids = {
        'scheduler': main,
        'idle_vms': check_view_idle_vms,
    #    'vm_termination': vm_termination,
        'vm_termination': [vm_termination, 'select distinct group_name,cloud_name from csv2_clouds;'],
    }

    procMon = ProcessMonitor(
        config_params=[os.path.basename(sys.argv[0]), "csmain", 'ProcessMonitor', 'general', 'condor_poller.py'],
        pool_size=15,
        process_ids=process_ids
        )

    config = procMon.get_config()
    log = procMon.get_logging()
    version = config.get_version()

    PID_FILE = config.categories["ProcessMonitor"]["pid_path"] + os.path.basename(sys.argv[0])
    with open(PID_FILE, "w") as fd:
        fd.write(str(os.getpid()))

    log.info("****************************"
             " starting CSv2 Scheduler processes - Running %s "
             "*********************************", version)

    # Wait for keyboard input to exit
    try:
        # start processes
        procMon.start_all()
        signal.signal(signal.SIGTERM, terminate)
        while True:
            config.refresh()
            config.update_service_catalog()
            stop = check_pid(PID_FILE)
            procMon.check_processes(stop=stop)
            time.sleep(config.categories['ProcessMonitor']['sleep_interval_main_long'])

    except (SystemExit, KeyboardInterrupt):
        log.error("Caught KeyboardInterrupt, shutting down threads and exiting...")

    except Exception as ex:
        log.exception("Process Died: %s", ex)

    procMon.join_all()
